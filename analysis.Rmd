# 

Add topic background and references, overview of goals, description of the datasets used

First, let's import the necessary libraries

```{r, message=FALSE}
# load necessary libraries
library(sf)
library(terra)
library(here)
library(dplyr)
library(rpart)
library(rpart.plot)
library(tmap)

# clearing the environment for a fresh start
rm(list = ls())

```

### Data Processing and Preparation

This section involves preparing the Landsat data and study area for analysis.

First, let's create a raster stack using the 6 bands we will be working with in this analysis, update the layer names, and plot a true color image to visualize the data.

```{r}
# list the files for eahc band
filelist <- list.files(here("data/landsat-data"), full.names = TRUE)

# read in and store raster stack
landsat_20070925 <- rast(filelist)

# update layer names to match the bands 
names(landsat_20070925) <- c("blue", "green", "red", "NIR", "SWIR1", "SWIR2")

# plot true color image
plotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = "lin")

```

Looks good! This shows a view of the Santa Barbara region we will be analyzing.

We want to constrain the analysis to the southern part of Santa Barbara county since this is where the training data comes from, so let's proceed by reading in this data.

```{r}
# read in shapefile of southern SB county
study_area <- st_read(here("data/SB_county_south.shp"))

# reproject to match CRS of the Landsat data
study_area <- st_transform(study_area, crs = crs(landsat_20070925))
```

Nice. Now, cropping the Landsat data to the study area will create for a more concise visualization and also saves computational capacity. ADD SOMETHING ABOUT MASKING

```{r}
# crop Landsat to the extent of SB county
landsat_cropped <- crop(landsat_20070925, study_area)

# mask the raster 
landsat_masked <- mask(landsat_cropped, study_area)

# clean up environment by removing unecessary object
rm(landsat_20070925, study_area, landsat_cropped)
```

### Reflectance Conversion 

The next step in the analysis involves converting the raster sstack values to correspond to reflectance values. This entails removing potentially erroneous values and applying a scale factor to convert to reflectance. ADD A SENTENCE ON WHAT THIS MEANS.

We are working with Landsat data, which has a valid range of pixel values that have a multiplicative scale factor of 0.0000275 and a scale factor of -0.2. This piece of analysis will label erroneous values as NA and update the values for each pixel based on the scaling factor conversion. This will generate pixel values from 0 to 100 percent.

```{r}
# reclassify erroneous values as NA
rcl <- matrix(c(-Inf, 7273, NA,
                 43636, Inf, NA), ncol = 3, byrow = TRUE)

landsat <- classify(landsat_masked, rcl = rcl)

# adjust values according to scale 
landsat <- (landsat * 0.0000275 - 0.2) * 100

# plot true color image to verify results
plotRGB(landsat, r = 3, g = 2, b = 1, stretch = "lin")

# confirm that the values are from 0 to 100
summary(landsat)

```

Looks good! We can move on to the next section of the analysis.

### Training Data Extraction and Model Training

Next, we will load in the study area shapefile that contains each of the four land cover types. Then, the spectral values at each site will be extracted to create a dataframe that pairs land cover types to spectral reflectance. ADD MORE. DISCUSS LEFT JOIN

```{r}
# read in and transform CRS of training data
training_data <- st_read(here("data/trainingdata.shp")) %>% 
  st_transform(., crs = crs(landsat))

# extract reflectance values of training area
training_data_vals <- extract(landsat, training_data, df = TRUE)

# convert training data to a dataframe
training_data_df <- training_data %>% 
  st_drop_geometry()

# join training data attributes with reflectance values
sb_training_data <- left_join(training_data_vals, training_data_df,
                              by = c("ID" = "id")) %>% 
  mutate(type = as.factor(type)) # convert Landvoer to a factor
```

Great! Now that we have our training data sorted, we need to establish the response and predictor variables for the next step of the analysis. This will involve using the `rpart` function, which takes in the model formula and training data. This analysis method is a classification, so we will set `method = class` and `na.action=na.omit` to remove NA values.

Let's build out the decison tree that will be used to classify the pixels. Each decision rule has two possible outcomes based on a conditional statement that is the value of each spectral band. This tree is therefore a binary tree.

```{r}
# create model formula
sb_formula <- type ~ red + green + NIR + SWIR1 + SWIR2

# train the decision tree
sb_tree <- rpart(formula = sb_formula,
                 data = sb_training_data,
                 method = "class",
                 na.action = na.omit)

# visualize decision tree 
prp(sb_tree)
```

Neat! ADD HOW THIS CAN BE INTERPRETED.

### Image Classification and Results

Now, I will apply the decision tree to the entire image with the help of the `terra` package. Using the `predict()` function, we can get a raster layer of integer values corresponding to the factor levels in the training data. In order to determine which category each integer is for, we can inspect the training data.

```{r}
# classify the image based on decision tree results
sb_class <- predict(landsat, sb_tree, type = "class", na.rm = TRUE)

# inspect level to undersstand the order of prediction classes
levels(sb_training_data$type)
```

Great, this output makes sense. ADD MORE ABOUT LEVELS OUTPUT.

Now, let's plot the final results and check out the map.

```{r}
# plot results
tm_shape(sb_class) +
  tm_raster(col.scale = tm_scale_categorical(values = c("#8DB580", "#F2DDA4", "#7E8987", "#6A8EAE")),
            col.legend = tm_legend(labels = c("green vegetation", "soil/dead grass", "urban", "water"),
                                   title = "Landcover type")) +
  tm_layout(legend.position = c("left", "bottom"))
            
```
