---
title: "Title"
author: "Maxwell Patterson"
date: "`r Sys.Date()`"
---

### Overview

In our quest to understand the evolving landscape of southern Santa Barbara County, we turn to the precision of remote sensing. Through this, we aim to map the land cover accurately using the Landsat satellite's capabilities to discern distinct material types by their spectral signatures.

Our approach is methodical, employing a supervised classification technique with a decision tree classifier---a tool that learns to identify land cover categories based on examples we provide. This is a deliberate and informed process, where we train our model with specific data points that are known quantities, thereby allowing the algorithm to apply these learnings to classify a larger area.

The end goal is straightforward: to create a clear and accurate land cover classification map of the study area. This map will serve as a factual foundation for monitoring environmental changes and guiding decisions in land management and policy. By ensuring each pixel in the Landsat imagery is correctly classified, we aspire to produce a map that is as informative as it is functional."

#### Data

In our analysis, we utilize imagery from the Landsat 5 Thematic Mapper, specifically selecting a scene from September 25, 2007, which encompasses bands 1 through 5 and 7. This data is part of the Collection 2 surface reflectance product, ensuring a consistent and calibrated measure of the Earth's surface reflectivity.

The study's geographical focus is on the southern region of Santa Barbara County, delineated by a polygonal boundary within which our analysis is contained. Supplementing this are additional polygons that mark the training sites, each tagged with a specific land cover type as a character string. This training data is fundamental to informing the supervised classification process, allowing the decision tree model to learn and thereafter predict land cover types across the broader landscape.

ADD CITATION

First, let's import the necessary libraries

```{r, message=FALSE}
# load necessary libraries
library(sf)
library(terra)
library(here)
library(dplyr)
library(rpart)
library(rpart.plot)
library(tmap)

# clearing the environment for a fresh start
rm(list = ls())

```

### Data Processing and Preparation

This section involves preparing the Landsat data and study area for analysis.

First, let's create a raster stack using the 6 bands we will be working with in this analysis, update the layer names, and plot a true color image to visualize the data.

```{r}
# list the files for eahc band
filelist <- list.files(here("data/landsat-data"), full.names = TRUE)

# read in and store raster stack
landsat_20070925 <- rast(filelist)

# update layer names to match the bands 
names(landsat_20070925) <- c("blue", "green", "red", "NIR", "SWIR1", "SWIR2")

# plot true color image
plotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = "lin")

```

Looks good! This shows a view of the Santa Barbara region we will be analyzing.

We want to constrain the analysis to the southern part of Santa Barbara county since this is where the training data comes from, so let's proceed by reading in this data.

```{r}
# read in shapefile of southern SB county
study_area <- st_read(here("data/SB_county_south.shp"), quiet = TRUE)

# reproject to match CRS of the Landsat data
study_area <- st_transform(study_area, crs = crs(landsat_20070925))
```

Nice. Now, focusing our analysis on a specific geographic region is key to obtaining more relevant and meaningful insights. Cropping the Landsat data to our study area, specifically southern Santa Barbara County, provides a more concise visualization by directly targeting the area of interest. This step is critical for economizing computational resources, ensuring we're working with data that's most pertinent to our objectives without overburdening our processing capabilities.

Moreover, the process of masking is an essential next step. After cropping the Landsat data, masking helps us to further refine our dataset. By applying this mask, we effectively focus on only the data points that fall within the boundaries of our study area. This precision is crucial as it eliminates any extraneous information that might obscure our analysis, ensuring that every bit of data we examine is directly related to our study area. It's about enhancing the specificity of our dataset, ensuring that our analysis is as targeted and relevant as possible.

To keep the workspace organized and efficient, we also perform some necessary housekeeping. Removing objects like the original Landsat data and the intermediate cropped version helps maintain a streamlined working environment. It's all about focus and efficiency -- by keeping our workspace clutter-free, we pave the way for a more insightful and focused analysis.

```{r}
# crop Landsat to the extent of SB county
landsat_cropped <- crop(landsat_20070925, study_area)

# mask the raster 
landsat_masked <- mask(landsat_cropped, study_area)

# clean up environment by removing unecessary object
rm(landsat_20070925, study_area, landsat_cropped)
```

### Reflectance Conversion

The next step in the analysis involves converting the raster stack values to correspond to reflectance values. This entails removing potentially erroneous values and applying a scale factor to convert to reflectance. This process is critical as it ensures that the satellite image data accurately represent the reflectivity of different surfaces on the earth. Reflectance values are pivotal in distinguishing between various land cover types because each type reflects light in a unique way. By converting to reflectance, we're effectively translating raw satellite data into a format that mirrors how light interacts with different surfaces on the ground.

We are working with Landsat data, which has a valid range of pixel values that have a multiplicative scale factor of 0.0000275 and a scale factor of -0.2. This piece of analysis will label erroneous values as NA and update the values for each pixel based on the scaling factor conversion. By doing so, we're filtering out any data points that don't fit within the expected range of values, ensuring the integrity and reliability of our dataset. The scaling factor adjustment is a crucial step in normalizing the data, allowing us to compare and analyze different regions accurately. After this conversion, the pixel values will represent percentages, ranging from 0 to 100 percent, reflecting a complete spectrum of possible reflectance. This range provides a standardized framework for analyzing the satellite imagery, enabling us to derive meaningful and accurate interpretations of the land cover.

```{r}
# reclassify erroneous values as NA
rcl <- matrix(c(-Inf, 7273, NA,
                 43636, Inf, NA), ncol = 3, byrow = TRUE)

landsat <- classify(landsat_masked, rcl = rcl)

# adjust values according to scale 
landsat <- (landsat * 0.0000275 - 0.2) * 100

# plot true color image to verify results
plotRGB(landsat, r = 3, g = 2, b = 1, stretch = "lin")

# confirm that the values are from 0 to 100
summary(landsat)

```

Looks good! We can move on to the next section of the analysis.

### Training Data Extraction and Model Training

In this phase, we load and align the shapefile containing predefined land cover types with the Landsat data. This alignment is critical to ensure spatial consistency for accurate extraction of spectral values. We then extract these values from the Landsat imagery at locations marked in the training data, creating a dataframe that links each land cover type with its corresponding spectral reflectance.

By performing a `left_join`, we merge these spectral values with additional attributes from the training data. This step enriches our dataset, combining essential spectral information with land cover classifications. Finally, we convert the land cover types into factors, preparing the dataset for effective use in the decision tree classification algorithm. This process sets the stage for training a model that can accurately classify the entire Landsat scene based on the relationships it learns between spectral data and land cover types.

```{r}
# read in and transform CRS of training data
training_data <- st_read(here("data/trainingdata.shp"), quiet = TRUE) %>% 
  st_transform(., crs = crs(landsat))

# extract reflectance values of training area
training_data_vals <- extract(landsat, training_data, df = TRUE)

# convert training data to a dataframe
training_data_df <- training_data %>% 
  st_drop_geometry()

# join training data attributes with reflectance values
sb_training_data <- left_join(training_data_vals, training_data_df,
                              by = c("ID" = "id")) %>% 
  mutate(type = as.factor(type)) # convert Landvoer to a factor
```

Great! Now that we have our training data sorted, we need to establish the response and predictor variables for the next step of the analysis. This will involve using the `rpart` function, which takes in the model formula and training data. This analysis method is a classification, so we will set `method = class` and `na.action=na.omit` to remove NA values.

Let's build out the decison tree that will be used to classify the pixels. Each decision rule has two possible outcomes based on a conditional statement that is the value of each spectral band. This tree is therefore a binary tree.

```{r}
# create model formula
sb_formula <- type ~ red + green + NIR + SWIR1 + SWIR2

# train the decision tree
sb_tree <- rpart(formula = sb_formula,
                 data = sb_training_data,
                 method = "class",
                 na.action = na.omit)

# visualize decision tree 
prp(sb_tree)
```

Neat! The decision tree generated from the provided code offers a visual representation of the classification rules derived from the training data. Starting at the root, the tree uses the Near-Infrared (NIR) spectral band to make the initial split: if NIR reflectance is equal to or greater than 4.2, we move to the left side of the tree, otherwise, we classify the pixel as 'water'.

On the left branch, the tree further refines the classification by considering the red band. If red reflectance is less than 7.4, the decision process continues down this path, eventually utilizing the green band and NIR again to distinguish between 'green_ve' (green vegetation) and 'soil_dea' (soil or dead grass). If the red reflectance is greater than or equal to 7.4, the Short-Wave Infrared 1 (SWIR1) band is used to differentiate 'urban' areas from 'soil_dea' or 'urban' again, depending on the green band's reflectance value.

This decision tree model is intuitive, as it mirrors the natural process of elimination and categorization that a human expert might use, considering one variable at a time. NIR is typically associated with vegetation, as healthy plants reflect more NIR than other land cover types. The red band is often used to detect chlorophyll absorption, thus indicating vegetation. The SWIR bands are useful for distinguishing between moisture levels in soil and vegetation, and can also be indicative of urban materials. By applying these successive decisions based on spectral reflectance values, we can systematically classify the landscape into the four desired land cover categories.

### Image Classification and Results

Now, I will apply the decision tree to the entire image with the help of the `terra` package. Using the `predict()` function, we can get a raster layer of integer values corresponding to the factor levels in the training data. In order to determine which category each integer is for, we can inspect the training data.

```{r}
# classify the image based on decision tree results
sb_class <- predict(landsat, sb_tree, type = "class", na.rm = TRUE)

# inspect level to undersstand the order of prediction classes
levels(sb_training_data$type)
```

This output reveals the categorical levels that the decision tree model has used to classify the land cover types. The four levels---"green_vegetation," "soil_dead_grass," "urban," and "water"---correspond to the land cover classes present in our training data. These classes will be used to label the pixels in the classified image, assigning each pixel to one of these categories based on the spectral data and the classification rules learned during the model training phase. The order of these levels is significant as it relates directly to the integer values that will be used in the classified raster, providing a clear reference for interpreting the final land cover map.

Now, let's plot the final results and check out the map.

```{r}
# plot results
tm_shape(sb_class) + 
  tm_raster(
    palette = c("#8DB580", "#F2DDA4", "#7E8987", "#6A8EAE"),
    labels = c("green vegetation", "soil/dead grass", "urban", "water"),
    title = "Classification"
  ) +
  tm_layout(
    legend.position = c("left", "bottom"),
    title = "Santa Barbara Land Classification",
    title.position = c("right", "top"),
    title.size = 1
  )
```
